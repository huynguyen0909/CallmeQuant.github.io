---
title: "Jensen's Inequality and Kullback-Leibler Divergence"
date: 2023-01-28
categories:
  - Study
tags:
  - Bayesian
  - Machine Learning
  - Statistics
---

In the [post](https://callmequant.github.io/study/variational-inference/) about variational inference, we used many applications 
of the concepts from relative information theory (Kullback-Leibler divergence or KL divergence for short). KL divergence is a standard metric
to measure and quantifying the proximity between distributions and its role in density estimation is immense.
This post is an attempt to re-derive some useful properties of KL divergence such as non-negativity and further
provide the essence of Jensen's inequality in the derivation of these characteristics.

# Introduction
Kullback-Leibler divergence emerges frequently in the context of statistics which is a common approach to measure the similarity of two distributions. The ususal formulation is provided and the one I encountered most in many machine learning and deep learning textbooks is the following expectation (or integral)

$$
\begin{align}
\operatorname{KL}[Q || P] & = \mathbb{E}_{q(x)} [\operatorname{log}q(x) - \operatorname{log}p(x)],\\
& = \int^{\infty}_{-\infty} q(x) \operatorname{log} \frac{q(x)}{p(x)}\, dx.
\end{align}
$$ 

When we mention a metric for similarity measurement, we 



# Non-negativity of Kullback-Leibler Divergence 
As in variational inference, we claim that the KL divergence is always greater or equal to zero. A more implicit reason would be the KL d











