---
title: "Jensen's Inequality and Kullback-Leibler Divergence"
date: 2023-01-28
categories:
  - Study
tags:
  - Bayesian
  - Machine Learning
  - Statistics
---

In the [post](https://colab.research.google.com/drive/1g61AxatS8rfrA55jBh9uUvRBY2ZL6LhQ?usp=sharing) about variational inference, we used many applications 
of the concepts from relative information theory (Kullback-Leibler divergence or KL divergence for short). KL divergence is a standard metric
to measure and quantifying the proximity between distributions and its role in density estimation is immense.
This post is an attempt to re-derive some useful properties of KL divergence such as non-negativity and further
provide the essence of Jensen's inequality in the derivation of these characteristics.

# Introduction
