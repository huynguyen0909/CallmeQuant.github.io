---
title: "Maximum A Posterior"
date: 2023-06-14
mathjax: true
toc: true
categories:
  - Study
tags:
  -  Bayesian Inference
  -  Statistics
---
It is well-known that There are two ways of evaluating parameters 
commonly used in statistical machine learning. The first method is 
based solely on known data in the training data, called Maximum 
Likelihood Estimation or ML Estimation or MLE. The second method is 
based not only on training data but also on the known information of 
parameters. This information can be obtained by the sense of the model
builder. The clearer the senses, the more rational, the higher the
likelihood of obtaining a good set of parameters. For example, in the coin tossing
problem, given the parameter of interest $\theta$ being the probability of 
obtaining a head, we can expect this parameter's value should be close to $0.5$. 
This second approach for learning and assessing parameters is called *Maximum 
A Posteriori Estimation* or *MAP Estimation*. Despite some differences, the mathematical 
structures underpinning behind the two methods forge a connection between them.
In this article, I will present the idea and how to solve the problem of evaluating model parameters according to MLE or MAP Estimation. 
And as always, we'll go through a few simple examples.

# Maximum a Posteriori
## Why needs MAP?
A key issue with MLE is that they aim to opt for parameters that minimize the loss function over the training data. However, this may not result in a model that can perform well on future data, which is called *overfitting*. A simple example to illustrate the failure of MLE in *generalizing* ability is again the coin tossing problem. 

Suppose that you toss the coin $5$ times and and observe $5$ head faces, according to the MLE, the chance of attaining a head would be 
$\theta_{MLE} = \frac{N_{\text{head}}}{N_{\text{tail}} + N_{\text{head}}} = \frac{5}{5} = 1$. It would be too unrealistic to conclude such result  since we have too little data (corresponding to the phenomenon of having too little data in training . The key issue concerned with *low-training* data is that the training data may not be representative for the *true distribution* that we are trying to inference; thereby, the perfectly fitted model on the training data may not generalize well enough when future data is coming. One common remedy to this problem is to deduce some assumptions of the parameters. For example, with a coin toss, our assumption is that the probability of getting the head should be approximately close to $0.5$.
## MAP Formulation
Maximum A Posteriori (MAP) was devised to tackle this problem. In MAP, we introduce a known assumption, called a *prior*, of the parameter $\theta$. Based on our *previous experience*, we can deduce the distributions of such prior on parameters. 

In constrast of MLE, which use the likelihood (in other words, the joint distribution of the data and the parameter), MAP evaluate the parameter as a conditional probability of the data:

$$\theta = \underset{\theta}{\mathrm{argmax}}\ \underbrace{P(\theta \lvert x_1, x_2, \dots, x_N)}_{\text{posterior}} \tag{1}$$

The expression of the maximization problem $P(\theta \lvert x_1, x_2 \dots, x_N)$ is generally known as *posterior* probability of $\theta$. This is also the reason why this method is named *Maximum A Posteriori*. 

It is easy to see the connection with the Bayesian context. Bayesian analysis commences from formulating the link between the posterior, the likelihood, the prior and the evidence. The objective of Bayesian inference is to estimate the posterior distribution (which is often intractable), by taking the product of likelihood and the prior and often ignore the evidence since it does not depend on the parameter $\theta$. Denote a sample $(x_1, x_2, \dots, x_N)$ is drawn from random variable $X$. Then,

$$
\begin{align} \theta_{MAP} &= \mathop{\rm arg\,max}\limits_{\theta} P(X \vert \theta) P(\theta) \\ 
&= \mathop{\rm arg\,max}\limits_{\theta} \log P(X \vert \theta) + \log P(\theta) \\ 
&= \mathop{\rm arg\,max}\limits_{\theta} \log \prod_{i = 1}^n P(x_i \vert \theta) + \log P(\theta) \\ 
&= \mathop{\rm arg\,max}\limits_{\theta} \sum_{i = 1}^n \log P(x_i \vert \theta) + \log P(\theta) \end{align} \tag{2}$$

The last expression is identical to that of MLE, except for the inclusion of the term $\log{P(\theta)}$ - the log prior of the parameter $\theta$. In simpler terms, if we define a prior distribution for the model parameter, the likelihood is influenced not only by the likelihood of each data point but also by the prior. Think of the prior as an extra “constraint” in a broad sense. The best parameter must fit the data and also not stray too far from the prior





