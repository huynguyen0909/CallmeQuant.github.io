---
title: "Maximum A Posterior"
date: 2023-06-14
mathjax: true
toc: true
categories:
  - Study
tags:
  -  Bayesian Inference
  -  Statistics
---
It is well-known that There are two ways of evaluating parameters 
commonly used in statistical machine learning. The first method is 
based solely on known data in the training data, called Maximum 
Likelihood Estimation or ML Estimation or MLE. The second method is 
based not only on training data but also on the known information of 
parameters. This information can be obtained by the sense of the model
builder. The clearer the senses, the more rational, the higher the
likelihood of obtaining a good set of parameters. For example, in the coin tossing
problem, given the parameter of interest $\theta$ being the probability of 
obtaining a head, we can expect this parameter's value should be close to $0.5$. 
This second approach for learning and assessing parameters is called *Maximum 
A Posteriori Estimation* or *MAP Estimation*. Despite some differences, the mathematical 
structures underpinning behind the two methods forge a connection between them.
In this article, I will present the idea and how to solve the problem of evaluating model parameters according to MLE or MAP Estimation. 
And as always, we'll go through a few simple examples.

