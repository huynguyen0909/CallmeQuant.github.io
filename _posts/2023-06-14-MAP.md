---
title: "Maximum A Posterior"
date: 2023-06-14
mathjax: true
toc: true
categories:
  - Study
tags:
  -  Bayesian Inference
  -  Statistics
---
It is well-known that There are two ways of evaluating parameters 
commonly used in statistical machine learning. The first method is 
based solely on known data in the training data, called Maximum 
Likelihood Estimation or ML Estimation or MLE. The second method is 
based not only on training data but also on the known information of 
parameters. This information can be obtained by the sense of the model
builder. The clearer the senses, the more rational, the higher the
likelihood of obtaining a good set of parameters. For example, in the coin tossing
problem, given the parameter of interest $\theta$ being the probability of 
obtaining a head, we can expect this parameter's value should be close to $0.5$. 
This second approach for learning and assessing parameters is called *Maximum 
A Posteriori Estimation* or *MAP Estimation*. Despite some differences, the mathematical 
structures underpinning behind the two methods forge a connection between them.
In this article, I will present the idea and how to solve the problem of evaluating model parameters according to MLE or MAP Estimation. 
And as always, we'll go through a few simple examples.

# Maximum a Posteriori
## Why needs MAP?
A key issue with MLE is that they aim to opt for parameters that minimize the loss function over the training data. However, this may not result in a model that can perform well on future data, which is called *overfitting*. A simple example to illustrate the failure of MLE in *generalizing* ability is again the coin tossing problem. 

Suppose that you toss the coin $5$ times and and observe $5$ head faces, according to the MLE, the chance of attaining a head would be 
$\theta_{MLE} = \frac{N_{\text{head}}}{N_{\text{tail}} + N_{\text{head}}} = \frac{5}{5} = 1$. It would be too unrealistic to conclude such result  since we have too little data (corresponding to the phenomenon of having too little data in training . The key issue concerned with *low-training* data is that the training data may not be representative for the *true distribution* that we are trying to inference; thereby, the perfectly fitted model on the training data may not generalize well enough when future data is coming. One common remedy to this problem is to deduce some assumptions of the parameters. For example, with a coin toss, our assumption is that the probability of getting the head should be approximately close to $0.5$.
## MAP Formulation
Maximum A Posteriori (MAP) was devised to tackle this problem. In MAP, we introduce a known assumption, called a *prior*, of the parameter $\theta$. Based on our *previous experience*, we can deduce the distributions of such prior on parameters. 

In constrast of MLE, which use the likelihood (in other words, the joint distribution of the data and the parameter), MAP evaluate the parameter as a conditional probability of the data:

$$\theta = \underset{\theta}{\mathrm{argmax}}\ \underbrace{P(\theta \lvert x_1, x_2, \dots, x_N)}_{\text{posterior}} \tag{1}$$

The expression of the maximization problem $P(\theta \lvert x_1, x_2 \dots, x_N)$ is generally known as *posterior* probability of $\theta$. This is also the reason why this method is named *Maximum A Posteriori*. 

It is easy to see the connection with the Bayesian context. Bayesian analysis commences from formulating the link between the posterior, the likelihood, the prior and the evidence. The objective of Bayesian inference is to estimate the posterior distribution (which is often intractable), by taking the product of likelihood and the prior and often ignore the evidence since it does not depend on the parameter $\theta$. Denote a random sample $(x_1, x_2, \dots, x_N)$ of a random variable $X$. Then,

$$
\begin{align} \theta_{MAP} &= \mathop{\rm arg\,max}\limits_{\theta} P(X \vert \theta) P(\theta) \\ 
&= \mathop{\rm arg\,max}\limits_{\theta} \log P(X \vert \theta) + \log P(\theta) \\ 
&= \mathop{\rm arg\,max}\limits_{\theta} \log \prod_{i = 1}^n P(x_i \vert \theta) + \log P(\theta) \\ 
&= \mathop{\rm arg\,max}\limits_{\theta} \sum_{i = 1}^n \log P(x_i \vert \theta) + \log P(\theta) \end{align} \tag{2}
$$

The last expression is identical to that of MLE, except for the inclusion of the term $\log{P(\theta)}$ - the log prior of the parameter $\theta$. In simpler terms, if we define a prior distribution for the model parameter, the likelihood is influenced not only by the likelihood of each data point but also by the prior. Think of the prior as an extra "constraint" in a broad sense. The best parameter must fit the data and also not stray too far from the prior. Normally, *prior* is usually chosen based on the pre-known information of the parameter, and the distribution chosen is usually *conjugate distributions* with likelihood, i.e. distributions that cause prior multiplication to retain the same structure as *likelihood*.

# Conjugate Prior
In Bayesian inference, conjugacy implies that the posterior $p(\theta \lvert X)$ has the same functional form as the prior $p(\theta)$. If such case occurs, we state that the *prior* $p(\theta)$ is *conjugate* to the likelihood function 
$p(X \lvert \theta)$. Specifically, if we look at the standard Bayes' formula, the following terms have the same functional form

$$\boxed{p(\theta \lvert X)} = \dfrac{p(X \lvert \theta) \boxed{p(\theta)}}{\int p(X \lvert \theta^\prime) p(\theta^\prime)\, d \theta^\prime}$$. 

Recalling the earlier example about the bias $\theta$ of a coin. For $N$ times flips, the outcome of the $i$th coin toss is $x_i$ belonging to a Bernoulli random variable $X_i$ that takes on value of $0$ and $1$ (tail or head respectively). Then, we have shown that the maximum likelihood estimator of $\theta$, denoted by $\hat{\theta}_{MLE} = \frac{N_{\text{head}}}{N_{\text{tail}} + N_{\text{head}}}$. In low-data case, this estimator might suffer from being overfitted. One remedy is to use MAP (or Bayesian inference) to impose a constraint (in this method it is the incorporation of prior knowledge about what $\theta$ normally is into the model). The question arises on the way to sort out the prior we should place on $\theta$. Conjugate prior is the key!

## Coin tossing in Bayesian paradigm

We still hold an assumption that the majority of coins is fair. Then even though we observe five heads in a row, we want to incorporate our prior knowledge of what the true probability of head typical is into our model. The remanining question lies in the choice of prior imposing on our parameter of interest $\theta$. Beginning with the two modeling assumptions made earlier. Suppose that most coins are fair. This implies that the mode of the distribution should be around $0.5$. Additionally, we assume that biased coins do not favor heads over tails. This indicates that we should choose a symmetric distribution. One distribution that may have these properties is the *beta* distribution, which is given by the following formula

$$\text{Beta}(\lambda \lvert \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \lambda^{\alpha - 1} ( 1 - \lambda) ^{\beta - 1}.$$

where $\Gamma(x)$ is the gamma function

$$ \Gamma(x) = \int_{0}^{\infty} \lambda^{x-1} \exp{-\lambda} d\lambda.$$

and the term $\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}$ is the normalization that ensures this is a proper distribution. That is, we can easily check that 

$$\int_{0}^{1} \lambda^{\alpha - 1} ( 1 - \lambda) ^{\beta - 1} d\lambda = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}.$$

